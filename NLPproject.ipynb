{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPproject.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IMvzRyL9SHQ",
        "outputId": "cf3d13a7-4de7-480c-e337-97691ab99142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import csv\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "mg1jDvNr-J1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Easy Cleaning\n",
        "\n",
        "# def preprocessing(raw_data):\n",
        "\n",
        "#   # remove non english tweets\n",
        "#   idx_to_delete = raw_data[raw_data[\"language\"] != \"en\"].index\n",
        "#   raw_data.drop(idx_to_delete, inplace=True)\n",
        "#   data = raw_data.filter([\"tweet\", \"hashtags\"])\n",
        "  \n",
        "#   data[\"tweet\"] = data[\"tweet\"].apply(lambda row: remove_hashtag_from_text(row))\n",
        "#   data[\"tweet\"] = data[\"tweet\"].apply(lambda row: remove_link_from_text(row))\n",
        "#   data[\"tweet\"] = data[\"tweet\"].apply(lambda row: remove_emoji_from_text(row))\n",
        "#   data[\"tweet\"] = data[\"tweet\"].apply(lambda row: remove_tag_from_text(row))\n",
        "#   return data\n"
      ],
      "metadata": {
        "id": "5pKjsZ45NXe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Medium Cleaning\n",
        "\n",
        "# def preprocessing(raw_data):\n",
        "\n",
        "#   # remove non english tweets\n",
        "#   idx_to_delete = raw_data[raw_data[\"language\"] != \"en\"].index\n",
        "#   raw_data.drop(idx_to_delete, inplace=True)\n",
        "#   data = raw_data.filter([\"tweet\", \"hashtags\"])\n",
        "  \n",
        "#   data[\"tweet\"] = data[\"tweet\"].apply(lambda row: remove_hashtag_from_last(row))\n",
        "#   data[\"tweet\"] = data[\"tweet\"].apply(lambda row: remove_link_from_text(row))\n",
        "#   # data[\"tweet\"] = data[\"tweet\"].apply(lambda row: remove_emoji_from_text(row))\n",
        "#   data[\"tweet\"] = data[\"tweet\"].apply(lambda row: remove_tag_from_text(row))\n",
        "#   return data\n"
      ],
      "metadata": {
        "id": "--sfExQDOgd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hard Cleaning\n",
        "\n",
        "def preprocessing(raw_data):\n",
        "\n",
        "  # remove non english tweets\n",
        "  idx_to_delete = raw_data[raw_data[\"language\"] != \"en\"].index\n",
        "  raw_data.drop(idx_to_delete, inplace=True)\n",
        "  data = raw_data.filter([\"tweet\", \"hashtags\"])\n",
        "  \n",
        "  # data[\"tweet\"] = data[\"tweet\"].apply(lambda row: remove_hashtag_from_last(row))\n",
        "  # data[\"tweet\"] = data[\"tweet\"].apply(lambda row: remove_link_from_text(row))\n",
        "  # data[\"tweet\"] = data[\"tweet\"].apply(lambda row: remove_emoji_from_text(row))\n",
        "  # data[\"tweet\"] = data[\"tweet\"].apply(lambda row: remove_tag_from_text(row))\n",
        "  return data"
      ],
      "metadata": {
        "id": "ABx3M1MHShTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_hashtag_from_text(text):\n",
        "  return re.sub(r'#\\w+', \"\", text)\n",
        "\n",
        "def remove_link_from_text(text):\n",
        "  return re.sub('http[s]?://\\S+', '', text)\n",
        "\n",
        "def remove_tag_from_text(text):\n",
        "  return re.sub(r'@\\w+', \"\", text)\n",
        "\n",
        "def remove_hashtag_from_last(text):\n",
        "  return re.sub(r'(#[^# ]+?)+$', '', text)\n",
        "        \n",
        "def remove_emoji_from_text(text):\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "         u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\" \"]+\", flags=re.UNICODE)\n",
        "  return emoji_pattern.sub(r'', text) # no emoji\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OWC1uHSu-yoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for file_name in list_of_csv:\n",
        "#   raw_data = pd.read_csv(\"/content/drive/MyDrive/CSCI-544/data/science\" + file_name, sep='\\t')\n",
        "#   data = preprocessing(raw_data)\n",
        "#   print(data)\n",
        "#   break\n",
        "\n",
        "  # new_data = data.rename(columns={'tweet': 'text', 'hashtags' : 'label'})\n",
        "  # name = \"cleaned_\" + file_name\n",
        "  # new_data.to_csv('/content/drive/MyDrive/CSCI-544/new_data/'+ name)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ukGkWndB-W8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# c_data = {'tweet':[],\n",
        "#         'hashtags':[]}\n",
        " \n",
        "# # Create DataFrame\n",
        "# complete_data = pd.DataFrame(c_data)\n",
        " \n",
        "# # Print the output.\n",
        "# complete_data"
      ],
      "metadata": {
        "id": "ynao_xDGNrFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = defaultdict(int)\n",
        "import glob\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "domains = ['science', 'sports' , 'politics' , 'finance' , 'entertainment']\n",
        "\n",
        "for i in domains:\n",
        "  \n",
        "  folder_name = i\n",
        "  #!ls \"/gdrive/My Drive/folder\"\n",
        "\n",
        "  files = glob.glob(f\"/content/drive/MyDrive/CSCI-544/data/\" + i + \"/*.csv\")\n",
        "  appended_data = []\n",
        "  for file in files:\n",
        "\n",
        "    raw_data = pd.read_csv( file, sep='\\t')\n",
        "    # print(raw_data)\n",
        "    data = preprocessing(raw_data)\n",
        "    appended_data.append(data)\n",
        "\n",
        "  appended_data = pd.concat(appended_data)\n",
        "  # print(appended_data)\n",
        "\n",
        "  temp = appended_data['hashtags'].values.tolist()\n",
        "  for i in temp:\n",
        "    res  = i.strip('][').split(', ')\n",
        "    for j in res:\n",
        "      dictionary[j]+=1\n",
        "\n",
        "  new_data = appended_data.rename(columns={'tweet': 'text', 'hashtags' : 'label'})\n",
        "  name = \"hard_cleaned_\" +  folder_name + '.csv'\n",
        "  new_data.to_csv('/content/drive/MyDrive/CSCI-544/new_data/'+ name)\n",
        "    "
      ],
      "metadata": {
        "id": "TvHFqfovyKPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aee7d3b-5ee0-430d-fce6-9dfa64eb4421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For counting total labels\n",
        "\n",
        "# dictionary\n",
        "# len(dictionary)\n",
        "# w = csv.writer(open(\"dictionary_sports.csv\", \"w\"))\n",
        "\n",
        "# # loop over dictionary keys and values\n",
        "# for key, val in dictionary.items():\n",
        "\n",
        "#     # write every key and value to file\n",
        "#     w.writerow([key, val])"
      ],
      "metadata": {
        "id": "amzkKuYiKPJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# science - 6971\n",
        "# entertainment - 10475\n",
        "# finance - 5309\n",
        "# politics - 8062\n",
        "# sports - 8283\n",
        "\n"
      ],
      "metadata": {
        "id": "oP8Ng7S9Mgb4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}